[
  {
    "objectID": "hire.html",
    "href": "hire.html",
    "title": "About Me",
    "section": "",
    "text": "For the last 16 years in the industry, I have helped many companies with their data needs:\nAdditionally, I have created various projects myself, including:"
  },
  {
    "objectID": "hire.html#common-issues-faced-by-my-clients",
    "href": "hire.html#common-issues-faced-by-my-clients",
    "title": "About Me",
    "section": "Common Issues Faced by My Clients:",
    "text": "Common Issues Faced by My Clients:\n\n“Everyone claims to be data-driven. I want my team to adopt this approach too, with everything scientifically proven. What are the best tools?” You might not need them at all.\n“I’m worried that without a data strategy, we won’t be able to grow. We need to build a data lake, warehouse, and establish data stewardship.” Don’t let marketing-driven FOMO distract you from core business.\n“Vector search is the new gold standard; we need it ASAP.” No, you don’t—or at least not with the latest shiny database that overpromises.\n“Our company is only three years old, but we’ve already changed our data stack four times. Now the tech team wants to switch again.” It might be time to thoroughly evaluate the trade-offs before making another move.\n“I don’t want to be just a middleman between the user and the OpenAI API.” Let’s brainstorm.\n“We can’t create our own models; we don’t have billions of dollars and access to half of the internet’s data.” You don’t need to.\n“We change prompts but have no idea how to measure the difference without A/B testing, which takes too long and slows us down.” Effective evaluation techniques are key to speeding up development on top of LLMs; you’re not alone in facing this challenge."
  },
  {
    "objectID": "hire.html#hiring-options",
    "href": "hire.html#hiring-options",
    "title": "About Me",
    "section": "Hiring options",
    "text": "Hiring options\n\nPersonal Data Therapy for a Founder\nI help you become more comfortable with data decisions on a day-to-day basis.\n\n\nGroup Data Therapy for a Team\nWe conduct brainstorms and business-tech dialogues to highlight issues, find solutions, and create strategies.\n\n\nSpecific Topic Training Session\nExpand your team’s knowledge in a requested area.\n\n\nProject-Based Consulting\nI join your team for the project and take responsibility for the results."
  },
  {
    "objectID": "hire.html#where-to-find-me",
    "href": "hire.html#where-to-find-me",
    "title": "About Me",
    "section": "Where to find me",
    "text": "Where to find me\n\n\nTwitter\n\n\nGitHub\n\n\nalex+consulting@zverianskii.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Zverianskii",
    "section": "",
    "text": "Hey, I am building things around AI for more than 15 years in businesses of diverse sizes, ranging from 200k MAU to 100mln MAU. I’ve engineered hundreds of real-time models, prepared data for an IPO, and built three companies from the ground up, with one successful exit. Read my stories on python, data, AI and startups!"
  },
  {
    "objectID": "posts/master_power_law.html",
    "href": "posts/master_power_law.html",
    "title": "Master Power Law To Avoid Disasters",
    "section": "",
    "text": "import scipy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/master_power_law.html#introduction",
    "href": "posts/master_power_law.html#introduction",
    "title": "Master Power Law To Avoid Disasters",
    "section": "Introduction",
    "text": "Introduction\nRemember “Long-Term Capital Management” story? In essence, they applied the Gaussian distribution for forecasting market volatility. According to the Gaussian distribution, the likelihood of an event occurring that is 10 standard deviations away from the mean is 1.3x10⁻²³. However, under a power law with an exponent of 2, this probability increases to 0.5%!\nReal-life proved they were wrong:\n\n\n\nwikimedia lctm pricing\n\n\nThis situation serves as a strong motivator to delve into more complex distributions than the Gaussian.\nI have been in the data analysis field for 15 years.\nDuring this time, I’ve helped companies of all sizes navigate risks, developed predictive models, and performed many AB-tests.\nBut do you want to know a secret?\nI actually use the same 3 techniques every time:"
  },
  {
    "objectID": "posts/master_power_law.html#technique-1-identifying-power-law-dynamic",
    "href": "posts/master_power_law.html#technique-1-identifying-power-law-dynamic",
    "title": "Master Power Law To Avoid Disasters",
    "section": "Technique 1: Identifying Power Law Dynamic",
    "text": "Technique 1: Identifying Power Law Dynamic\n\nPlot your data on a log-log scale. A straight line indicates a power law distribution.\nRemember that fewer points at the far right tail can make it tricky to draw definitive conclusions.\nUse your log-log plot to calculate the power law’s exponent.\n\nFollow these steps, and you’ll be able to identify power law distribution accurately.\nHere is an example with exponential distribution (non straight line on the left):\n\nnum_samples = 1000\ndat = sorted(scipy.stats.expon.rvs(1.5, size=num_samples))\nx = range(len(dat))\nfig, ax = plt.subplots(1,2, figsize=(15, 7))\ng = sns.distplot(dat, ax=ax[0])\ng.set_title(\"Original\")\ng = sns.scatterplot(x=np.log(x), y=np.log(dat), ax=ax[1])\ng.set_title(\"Log-log\")\ng.set(ylim=(min(np.log(dat)), None))\n\n\n\n\n\n\n\n\nBut with Powerlaw we have:\n\ndat = sorted(scipy.stats.powerlaw.rvs(0.1, size=num_samples))\nx = range(len(dat))\nfig, ax = plt.subplots(1,2, figsize=(15, 7))\ng = sns.distplot(dat, ax=ax[0])\ng.set_title(\"Original\")\ng = sns.scatterplot(x=np.log(x), y=np.log(dat), ax=ax[1])\ng.set_title(\"Log-log\")\ng.set(ylim=(min(np.log(dat)), None))"
  },
  {
    "objectID": "posts/master_power_law.html#technique-2-acknowledging-real-world-limits",
    "href": "posts/master_power_law.html#technique-2-acknowledging-real-world-limits",
    "title": "Master Power Law To Avoid Disasters",
    "section": "Technique #2: Acknowledging Real-World Limits",
    "text": "Technique #2: Acknowledging Real-World Limits\nWhen you identify Powerlaw but still need to perform AB-test, there are some tricks you can perform, one of them is to consider real-world constraints, like market collapses or physical boundaries. In such cases, a truncated power law distribution may be more accurate, and with a propper boundary it’s mean can even converge to normal.\nBy considering these factors and incorporating them into your analysis, you’ll achieve more realistic assessments.\nHere’s an example, suppose we’re examining annual incomes. For decision-making purposes, we’re primarily interested in the majority and not particularly focused on individuals with exceptionally high incomes. Therefore, we’ve set a cap:\n\nN = 1000\nnum_samples = 2000\ndat = scipy.stats.pareto.rvs(.8, size=(N, num_samples))\nd = dat.mean(axis=0)\ndat[dat &gt; 200] = 200\nfig, ax = plt.subplots(1,2, figsize=(12, 5))\ng = sns.histplot(d, ax=ax[0], bins=20)\ng.set_title(\"Mean distibution without cap\")\ng = sns.histplot(dat.mean(axis=0), ax=ax[1], bins=20)\ng.set_title(\"With cap\")\n\nText(0.5, 1.0, 'With cap')"
  },
  {
    "objectID": "posts/master_power_law.html#technique-3-limitations-of-historical-data",
    "href": "posts/master_power_law.html#technique-3-limitations-of-historical-data",
    "title": "Master Power Law To Avoid Disasters",
    "section": "Technique #3: Limitations of Historical Data",
    "text": "Technique #3: Limitations of Historical Data\nRealize that learning from history can be challenging if the data follows power law.\nRemember that more data often means more extreme values.\nAlways consider the possibility of more extreme events beyond what your current data shows.\nA power law is a type of heavy-tailed distribution, indicating that the likelihood of rare events is significantly higher compared to a Gaussian distribution. Depending on the circumstances, this detail may or may not be crucial. For instance, while analyzing A/B testing on button colors, it might be negligible. However, it becomes vital when modeling financial risks.\nHappy coding!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "My notes",
    "section": "",
    "text": "Speed Up Python with PyPy\n\n\n\n\n\n\npython\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMaster Power Law To Avoid Disasters\n\n\n\n\n\n\nmath\n\n\nclt\n\n\nab\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAssumtions of CLT\n\n\n\n\n\n\nmath\n\n\nclt\n\n\nab\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/CLT_assumptions.html",
    "href": "posts/CLT_assumptions.html",
    "title": "Assumtions of CLT",
    "section": "",
    "text": "The Central Limit Theorem serves as the foundation for many aspects of statistics, including the z-test, student’s t-test, and Welch’s t-test. But, how frequently do you consider its assumptions? Have you ever verified them?\nEven a minor deviation from these assumptions can result in a situation where the methodology becomes unusable.\nMathematical concepts are often written in a complex way that can be challenging to decipher and understand. That’s why my aim is to break down these core principles and present them to you in simple, clear English with examples.\nLet’s start with a definition from wikipedia:\nfrom IPython.display import display, Image as dImage\nfrom PIL import Image\nimport scipy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\npath=\"./resources/images/screen4.png\"\ndisplay(Image.open(path))\nit turns out 3 major assumptions are hidden in 8 symbols: “&lt;∞” and “i.i.d.”:"
  },
  {
    "objectID": "posts/CLT_assumptions.html#key-takeaways",
    "href": "posts/CLT_assumptions.html#key-takeaways",
    "title": "Assumtions of CLT",
    "section": "Key Takeaways:",
    "text": "Key Takeaways:\n\nInfinite variance occurs more frequently than you might think. Stay aware.\nIt’s important to understand the data generating process, as this can provide some level of guarantee not only for independence but also for identically distributed data.\nKnow your data. This knowledge is key to effective analysis and interpretation.\n\nHappy coding!"
  },
  {
    "objectID": "posts/speedup_python_with_pypy.html",
    "href": "posts/speedup_python_with_pypy.html",
    "title": "Speed Up Python with PyPy",
    "section": "",
    "text": "As an Analyst, you’re likely familiar with the traditional python development cycle:\n\nCompose some code.\nRun the programmed script.\nReview the outputs (via print statements, logs, etc.)\nRefine and optimize.\n\nBut when your execution time is sluggish, this cycle becomes a frustratingly slow process.\nThere’s a widespread myth that Python, by default, is slow, and little can be done to change that. But I’m here to dispel that myth.\nThe truth is, Python’s speed can be boosted by orders of magnitude without much difficulty. The key lies in understanding how to leverage the right tools and techniques, such as PyPy, to streamline your Python development process. Why PyPy, you ask? Well, because at times, a simple shift in the manner you execute scripts can significantly speed up the process.\nThe best part is, there’s no need to spend valuable time rewriting your code. It’s essentially a free lunch.\nYou start by writing your Python code, which is then compiled into an internal format known as “Python bytecode”. Each bytecode statement is subsequently translated into a language your specific machine can comprehend, one statement at a time.\n\n\nCode\nfrom IPython.display import display, Image as dImage\nfrom PIL import Image\nimport scipy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\n\nCode\npath=\"../resources/images/a5714f29-c749-419b-a73b-d48dae184a56.png\"\ndisplay(Image.open(path))\n\n\n\n\n\n\n\n\n\nAnd yes, when your code involves a loop the interpretation/translation process leads to significant repetition.\nThat’s where PyPy and its Just-In-Time (JIT) compiler come into play. Consider PyPy as a more advanced version of a bytecode executor. It’s like an observant supervisor monitoring the process. When it notices a fragment of code undergoing repeated translation, it leaps into action to optimize it.\nHow does it do this? PyPy employs tactics such as type placement and compilation to prevent unnecessary repetition, hence speeding up the execution process.\nIn essence, PyPy is a powerful tool that can help you eliminate redundancy and optimize your code’s performance. So, the next time you find your code stuck in the endless loop of translation, remember PyPy might just be the solution you need.\nHere is a quick example:\n\n\nCode\nimport random\n\nrandom.seed(1)\n\ndef main():\n    N = 1_000_000\n    res = [0]*N\n    for i in range(N):\n        dat = (random.randint(0,10) for x in range(100))\n        res[i] = sum(dat) / N\n    print(res[:10])\n\n\nIf we ran it with CPython we get:\ntime python main.py\n[0.000528, 0.000502, 0.000493, 0.000463, 0.000477, 0.000524, 0.000534, 0.000549, 0.000478, 0.000482]\npython main.py  19,25s user 0,02s system 99% cpu 19,271 total\nand the same with PyPy:\ntime python main.py\n[0.000528, 0.000502, 0.000493, 0.000463, 0.000477, 0.000524, 0.000534, 0.000549, 0.000478, 0.000482]\npython main.py  2,39s user 0,03s system 100% cpu 2,420 total\nThe results speak for themselves - we achieved a speed increase of nine times using PyPy. However, it’s important to note a couple of factors:\n\nIf your code primarily involves numpy operations, this optimization technique may not yield significant improvements.\nPyPy doesn’t fully support every element of CPython, meaning some functionalities may not work as intended.\n\nSo, what’s the takeaway here?\nIf your code is predominantly written in Python and comprises a good number of loops, you should definitely give PyPy a shot. It could result in easy gains and a considerable boost in efficiency.\nAnd the best part? The transition from CPython to PyPy is a breeze. In my experience, it took just two commands to make the switch.\nI genuinely hope that the insights shared here will prove beneficial in your coding journey, saving you precious time and making your development process more efficient.\nHappy coding!"
  }
]